{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac854239",
   "metadata": {},
   "source": [
    "# Urban Sound Classification using Deep Learning\n",
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "**Type of Learning**: Supervised Deep Learning\n",
    "\n",
    "**Algorithms**: Deep Neural Networks (MLP, CNN, LSTM) with Hyperparameter Tuning\n",
    "\n",
    "**Task**: Multi-class Classification of Urban Sounds\n",
    "\n",
    "This project focuses on classifying urban sounds into 10 different categories using deep learning approaches. The UrbanSound8K dataset contains 8732 labeled sound excerpts from urban environments, which we use to train and evaluate various neural network architectures.\n",
    "\n",
    "## 2. Motivation and Goal\n",
    "\n",
    "**Motivation**: Urban sound classification has important applications in:\n",
    "- Smart city monitoring and noise pollution analysis\n",
    "- Audio surveillance systems\n",
    "- Environmental sound recognition for IoT devices\n",
    "- Audio-based context awareness in mobile applications\n",
    "\n",
    "**Goal**: The goal is to investigate which of the three deep learning classification models we learned in class applies the best to audio tasks. My initial guess is RNNs, as audio is sequential by nature, but I am not sure whether that will be preserved by the preprocessing.\n",
    "\n",
    "## 3. Dataset Source and Citation\n",
    "\n",
    "**Dataset**: UrbanSound8K Dataset\n",
    "\n",
    "**Source**: [https://urbansounddataset.weebly.com/urbansound8k.html](https://urbansounddataset.weebly.com/urbansound8k.html)\n",
    "\n",
    "**Citation**:\n",
    "J. Salamon, C. Jacoby and J. P. Bello, \"A Dataset and Taxonomy for Urban Sound Research\",\n",
    "22nd ACM International Conference on Multimedia, Orlando SA, Nov. 2014.\n",
    "\n",
    "Dataset compiled by Justin Salamon, Christopher Jacoby and Juan Pablo Bello. All files are excerpts of recordings\n",
    "uploaded to www.freesound.org. Please see FREESOUNDCREDITS.txt in the repository for an attribution list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6981bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91328d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "DATASET_PATH = \"UrbanSound8K\"\n",
    "METADATA_FILE = os.path.join(DATASET_PATH, \"metadata\", \"UrbanSound8K.csv\")\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(METADATA_FILE)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Dataset shape:\", metadata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35e5c71",
   "metadata": {},
   "source": [
    "## 4. Dataset Description\n",
    "**Data Size**:\n",
    "- Total sound files: 8,732 excerpts\n",
    "- Classes: 10 urban sound categories\n",
    "- Typical length: <= 4 seconds duration\n",
    "- Format: WAV files organized in 10 folds for cross-validation\n",
    "\n",
    "The sampling rate, bit depth, and number of channels are the same as those of the original file uploaded to Freesound (and hence may vary from file to file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic dataset information\n",
    "print(\"Total number of audio files:\", len(metadata))\n",
    "print(\"Number of unique classes:\", metadata['class'].nunique())\n",
    "print(\"Classes:\", sorted(metadata['class'].unique()))\n",
    "\n",
    "# Display dataset structure\n",
    "display(metadata.head())\n",
    "\n",
    "# Display dataset columns\n",
    "print(\"Dataset columns:\", metadata.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf69179",
   "metadata": {},
   "source": [
    "## 5. EXPLORATORY DATA ANALYSIS (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8993460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Label Distribution Analysis\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Class distribution\n",
    "plt.subplot(2, 2, 1)\n",
    "class_counts = metadata['class'].value_counts()\n",
    "sns.barplot(x=class_counts.values, y=class_counts.index, palette='viridis')\n",
    "plt.title('Distribution of Sound Classes')\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.ylabel('Class')\n",
    "\n",
    "# Fold distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "fold_counts = metadata['fold'].value_counts().sort_index()\n",
    "sns.barplot(x=fold_counts.index, y=fold_counts.values, palette='coolwarm')\n",
    "plt.title('Distribution of Samples Across Folds')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Number of Samples')\n",
    "\n",
    "# Duration statistics\n",
    "plt.subplot(2, 2, 3)\n",
    "durations_by_class = []\n",
    "class_names = []\n",
    "for class_name in sorted(metadata['class'].unique()):\n",
    "    class_records = metadata[metadata['class'] == class_name]\n",
    "    class_durations = class_records['end'] - class_records['start']\n",
    "    durations_by_class.append(class_durations)\n",
    "    class_names.append(class_name)\n",
    "\n",
    "plt.boxplot(durations_by_class, labels=class_names)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Duration Distribution by Class (Box Plot)')\n",
    "plt.ylabel('Duration (seconds)')\n",
    "plt.grid(True, alpha=.3)\n",
    "\n",
    "# Sample audio file analysis\n",
    "plt.subplot(2, 2, 4)\n",
    "# Analyze a sample file to show waveform diversity\n",
    "sample_file = metadata.sample(1, random_state=42).iloc[0]\n",
    "file_path = os.path.join(DATASET_PATH, \"audio\", f\"fold{sample_file['fold']}\", sample_file['slice_file_name'])\n",
    "audio, sr = librosa.load(file_path, sr=22050)\n",
    "time_axis = np.linspace(0, len(audio)/sr, len(audio))\n",
    "plt.plot(time_axis, audio, color='green', alpha=.7)\n",
    "plt.title('Sample Waveform: %s' % sample_file[\"class\"])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True, alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21dc7ad",
   "metadata": {},
   "source": [
    "### 5.1 Observations\n",
    "\n",
    "Most classes seem to have around 1000 samples a few with a significantly lower number of samples. Even then, this does not warrant resampling as the overall distribution is mostly balanced.\n",
    "\n",
    "Another important observation is that the folds do not contain the exact same number of samples. In fact, even if each contained the same number of samples, the *amount of data* would not be the same as the duration varies between samples. An important implication is thatfor a fair comparison, 10-fold cross-validation is necessary.\n",
    "\n",
    "It is also interesting to note that except for car horns and gun shots, all clips have a median of around 4.0. Gun shots have a median way below. While it is not used here, I suspect that including the duration as a feature/parameter while training would improve the scores of these two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65805631",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 5.2 Audio Characteristics Analysis\n",
    "\n",
    "# Analyze a few sample files to demonstrate audio properties\n",
    "sample_analysis = []\n",
    "for idx, row in metadata.sample(20, random_state=42).iterrows():\n",
    "    file_path = os.path.join(DATASET_PATH, \"audio\", f\"fold{row['fold']}\", row['slice_file_name'])\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    duration = len(audio) / sr\n",
    "    sample_analysis.append({\n",
    "        'class': row['class'],\n",
    "        'duration': duration,\n",
    "        'sample_rate': sr,\n",
    "        'samples': len(audio),\n",
    "        'max_amplitude': np.max(np.abs(audio))\n",
    "    })\n",
    "\n",
    "pd.DataFrame(sample_analysis).style.set_caption(\"Audio characteristics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Visualize Sample Audio Files\n",
    "def plot_sample_audios(metadata, n_samples=3):\n",
    "    \"\"\"Plot waveform and spectrogram for sample audio files\"\"\"\n",
    "    fig, axes = plt.subplots(n_samples, 3, figsize=(15, 4*n_samples))\n",
    "\n",
    "    sample_data = metadata.sample(n_samples, random_state=25)\n",
    "\n",
    "    for idx, (_, row) in enumerate(sample_data.iterrows()):\n",
    "        file_path = os.path.join(DATASET_PATH, \"audio\", f\"fold{row['fold']}\", row['slice_file_name'])\n",
    "\n",
    "        audio, sr = librosa.load(file_path, sr=22050)\n",
    "\n",
    "        # Waveform\n",
    "        axes[idx, 0].plot(np.linspace(0, len(audio)/sr, len(audio)), audio)\n",
    "        axes[idx, 0].set_title(\"Waveform: %s\" % row['class'])\n",
    "        axes[idx, 0].set_xlabel('Time (s)')\n",
    "        axes[idx, 0].set_ylabel('Amplitude')\n",
    "\n",
    "        # Spectrogram\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "        img = librosa.display.specshow(D, y_axis='log', x_axis='time', sr=sr, ax=axes[idx, 1])\n",
    "        axes[idx, 1].set_title(\"Spectrogram: %s\" % row['class'])\n",
    "        plt.colorbar(img, ax=axes[idx, 1])\n",
    "\n",
    "        # MFCCs\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "        librosa.display.specshow(mfccs, x_axis='time', ax=axes[idx, 2])\n",
    "        axes[idx, 2].set_title(\"MFCCs: %s\" % row['class'])\n",
    "        plt.colorbar(img, ax=axes[idx, 2])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot sample audio analysis\n",
    "plot_sample_audios(metadata, n_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2036388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Duration Analysis\n",
    "durations = metadata['end'] - metadata['start']\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(durations, bins=30, alpha=.7, color='lightblue', edgecolor='black')\n",
    "plt.title('Distribution of Audio Durations')\n",
    "plt.xlabel('Duration (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=durations)\n",
    "plt.title('Box Plot of Audio Durations')\n",
    "plt.xlabel('Duration (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Duration Statistics:\")\n",
    "print(\"Mean: %.2fs\" % durations.mean())\n",
    "print(\"Std: %.2fs\" % durations.std())\n",
    "print(\"Min: %.2fs\" % durations.min())\n",
    "print(\"Max: %.2fs\" % durations.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f2b89d",
   "metadata": {},
   "source": [
    "### 5.2 Observations\n",
    "\n",
    "As seen above, the data is extremely skewed. The tail to the left, while very weak, should be kept in mind when performing operations on the data, especially feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9edfc4",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing\n",
    "**Preprocessing Steps**:\n",
    "1. **Feature Extraction**: MFCC (Mel-Frequency Cepstral Coefficients) with delta and delta-delta features\n",
    "2. **Feature Aggregation**: Mean, standard deviation, and median across time frames\n",
    "3. **Feature Scaling**: StandardScaler for normalization\n",
    "4. **Label Encoding**: Convert class labels to numerical format\n",
    "5. **Data Splitting**: Predefined folds for training, validation, and testing\n",
    "\n",
    "Cropping or (random) sampling of waveform data was not performed as the clips are already very short (<= 4s).\n",
    "\n",
    "**Benefits of the preprocessing steps**:\n",
    "- MFCCs are well-established for audio classification as they capture perceptual frequency characteristics\n",
    "- Delta features capture temporal dynamics of the audio signal\n",
    "- Statistical aggregation reduces variable-length audio to fixed-length feature vectors\n",
    "- Standard scaling ensures stable training of neural networks\n",
    "- Fixed fold assignment ensures reproducible evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1f5674",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define fixed folds for model selection\n",
    "VAL_FOLD = 9\n",
    "TEST_FOLD = 10\n",
    "TRAIN_FOLDS = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "print(\"Fold Assignment:\")\n",
    "print(\"Training folds:\", TRAIN_FOLDS)\n",
    "print(\"Validation fold:\", VAL_FOLD)\n",
    "print(\"Test fold:\", TEST_FOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute features for all data once\n",
    "def precompute_all_features(metadata):\n",
    "    all_features = []\n",
    "    all_features_sequential = []\n",
    "    all_labels = []\n",
    "    all_folds = []\n",
    "\n",
    "    print(\"Precomputing features for all audio files...\")\n",
    "    for _, row in metadata.iterrows():\n",
    "        file_path = os.path.join(DATASET_PATH, \"audio\", f\"fold{row['fold']}\", row['slice_file_name'])\n",
    "\n",
    "        audio, sr = librosa.load(file_path, sr=22050)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40, n_fft=2048, hop_length=512)\n",
    "\n",
    "        mfccs_delta = librosa.feature.delta(mfccs, mode='nearest')\n",
    "        mfccs_delta2 = librosa.feature.delta(mfccs, order=2, mode='nearest')\n",
    "\n",
    "        features_combined = np.vstack([mfccs, mfccs_delta, mfccs_delta2])\n",
    "        features_aggregated = np.concatenate([\n",
    "            np.mean(features_combined, axis=1),\n",
    "            np.std(features_combined, axis=1),\n",
    "            np.median(features_combined, axis=1)\n",
    "        ])\n",
    "\n",
    "        features_sequential = features_combined.T\n",
    "\n",
    "        all_features.append(features_aggregated)\n",
    "        all_features_sequential.append(features_sequential)\n",
    "        all_labels.append(row['class'])\n",
    "        all_folds.append(row['fold'])\n",
    "\n",
    "    return np.array(all_features), all_features_sequential, np.array(all_labels), np.array(all_folds)\n",
    "\n",
    "# Precompute features for entire dataset\n",
    "all_features, all_features_sequential, all_labels, all_folds = precompute_all_features(metadata)\n",
    "print(\"Precomputed features shape:\", all_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f630e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data using precomputed features\n",
    "train_mask = np.isin(all_folds, TRAIN_FOLDS)\n",
    "val_mask = all_folds == VAL_FOLD\n",
    "test_mask = all_folds == TEST_FOLD\n",
    "\n",
    "X_train, y_train = all_features[train_mask], all_labels[train_mask]\n",
    "X_val, y_val = all_features[val_mask], all_labels[val_mask]\n",
    "X_test, y_test = all_features[test_mask], all_labels[test_mask]\n",
    "\n",
    "# Sequential features for LSTM\n",
    "X_train_seq = [feature for i, feature in enumerate(all_features_sequential) if train_mask[i]]\n",
    "X_val_seq = [feature for i, feature in enumerate(all_features_sequential) if val_mask[i]]\n",
    "X_test_seq = [feature for i, feature in enumerate(all_features_sequential) if test_mask[i]]\n",
    "\n",
    "print(\"Training set:\", X_train.shape[0], \"samples\")\n",
    "print(\"Validation set:\", X_val.shape[0], \"samples\")\n",
    "print(\"Test set:\", X_test.shape[0], \"samples\")\n",
    "\n",
    "# Verify class distribution\n",
    "print(\"\\nClass distribution across splits:\")\n",
    "print(\"\\nTraining:\", pd.Series(y_train).value_counts().sort_index(), sep='\\n')\n",
    "print(\"\\nValidation:\", pd.Series(y_val).value_counts().sort_index(), sep='\\n')\n",
    "print(\"\\nTest:\", pd.Series(y_test).value_counts().sort_index(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ccde65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(\"Class mapping:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    print(\"%s: %d\" % (class_name, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8da7aa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Scale sequential features for LSTM and pad to consistent length\n",
    "all_train_seq = np.vstack(X_train_seq)\n",
    "scaler_seq = StandardScaler()\n",
    "scaler_seq.fit(all_train_seq)\n",
    "\n",
    "X_train_seq_scaled = [scaler_seq.transform(seq) for seq in X_train_seq]\n",
    "X_val_seq_scaled = [scaler_seq.transform(seq) for seq in X_val_seq]\n",
    "X_test_seq_scaled = [scaler_seq.transform(seq) for seq in X_test_seq]\n",
    "\n",
    "# Pad sequences to consistent length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Find max sequence length\n",
    "max_length = max(len(seq) for seq in X_train_seq_scaled)\n",
    "print(f\"Maximum sequence length: {max_length}\")\n",
    "\n",
    "# Pad all sequences\n",
    "X_train_seq_padded = pad_sequences(X_train_seq_scaled, maxlen=max_length, dtype='float32', padding='post')\n",
    "X_val_seq_padded = pad_sequences(X_val_seq_scaled, maxlen=max_length, dtype='float32', padding='post')\n",
    "X_test_seq_padded = pad_sequences(X_test_seq_scaled, maxlen=max_length, dtype='float32', padding='post')\n",
    "\n",
    "print(f\"Padded training sequences shape: {X_train_seq_padded.shape}\")\n",
    "print(f\"Padded validation sequences shape: {X_val_seq_padded.shape}\")\n",
    "print(f\"Padded test sequences shape: {X_test_seq_padded.shape}\")\n",
    "\n",
    "# Reshape for CNN/MLP models\n",
    "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "X_val_reshaped = X_val_scaled.reshape(X_val_scaled.shape[0], X_val_scaled.shape[1], 1)\n",
    "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b3132",
   "metadata": {},
   "source": [
    "## 7. Model Selection and Architecture\n",
    "**Choice of Models**:\n",
    "1. **MLP (Multi-Layer Perceptron)**: Baseline model for comparison, good for structured feature data\n",
    "2. **CNN (Convolutional Neural Network)**: Effective for capturing local patterns in feature sequences\n",
    "3. **LSTM (Long Short-Term Memory)**: Suitable for sequenced structure of audio data\n",
    "\n",
    "The hyperparameter tuning from Keras Tuner uses bayesian optimization to find optimal parameters.\n",
    "\n",
    "The models (especially CNN and LSTM) are complex and therefore very time consuming. However, especially for MLP and CNN, they can be greatly accelerated by GPU so training feels almost instant, as long as the model is not too big to store in the GPU's memory. LSTM does take longer, but it remains tolerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c30f3a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_mlp_model(hp):\n",
    "    \"\"\"Build MLP model with tunable hyperparameters\"\"\"\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "\n",
    "    for i in range(hp.Int('num_layers', 2, 5)):\n",
    "        units = hp.Int(f'units_{i}', min_value=128, max_value=512, step=128)\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(hp.Float(f'dropout_{i}', .2, .6)))\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    optimizer_name = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_cnn_model(hp):\n",
    "    \"\"\"Build CNN model with tunable hyperparameters\"\"\"\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Reshape((input_dim, 1), input_shape=(input_dim,)))\n",
    "\n",
    "    for i in range(hp.Int('conv_layers', 2, 4)):\n",
    "        filters = hp.Int(f'filters_{i}', min_value=32, max_value=256, step=32)\n",
    "        kernel_size = hp.Int(f'kernel_{i}', min_value=3, max_value=7, step=2)\n",
    "        model.add(layers.Conv1D(filters, kernel_size, activation='relu', padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "\n",
    "    for i in range(hp.Int('dense_layers', 1, 3)):\n",
    "        units = hp.Int(f'dense_units_{i}', min_value=64, max_value=256, step=64)\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(hp.Float('dense_dropout', .3, .6)))\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    optimizer_name = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_lstm_model(hp):\n",
    "    \"\"\"Build LSTM model with tunable hyperparameters\"\"\"\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    # Use fixed input shape now that we have padded sequences\n",
    "    model.add(layers.Input(shape=(X_train_seq_padded.shape[1], X_train_seq_padded.shape[2])))\n",
    "    \n",
    "    # Always return sequences and use GlobalAveragePooling1D at the end\n",
    "    for i in range(hp.Int('lstm_layers', 1, 3)):\n",
    "        units = hp.Int(f'lstm_units_{i}', min_value=32, max_value=192, step=32)\n",
    "        model.add(layers.Bidirectional(layers.LSTM(units, return_sequences=True)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(hp.Float(f'lstm_dropout_{i}', .2, .5)))\n",
    "\n",
    "    # Always use GlobalAveragePooling1D to reduce from 3D to 2D\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    optimizer_name = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3927a59",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_model_specific_tuning(model_type, max_trials=15):\n",
    "    \"\"\"Run hyperparameter tuning for a specific model type\"\"\"\n",
    "\n",
    "    if model_type == 'mlp':\n",
    "        X_train_tune, X_val_tune = X_train_reshaped, X_val_reshaped\n",
    "        build_func = build_mlp_model\n",
    "    elif model_type == 'cnn':\n",
    "        X_train_tune, X_val_tune = X_train_reshaped, X_val_reshaped\n",
    "        build_func = build_cnn_model\n",
    "    elif model_type == 'lstm':\n",
    "        X_train_tune, X_val_tune = X_train_seq_padded, X_val_seq_padded\n",
    "        build_func = build_lstm_model\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        build_func,\n",
    "        objective='val_accuracy',\n",
    "        max_trials=max_trials,\n",
    "        directory='model_tuning',\n",
    "        project_name='urban_sound_'+model_type,\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, min_lr=1e-7)\n",
    "    ]\n",
    "\n",
    "    tuner.search(\n",
    "        X_train_tune, y_train_encoded,\n",
    "        epochs=50,\n",
    "        validation_data=(X_val_tune, y_val_encoded),\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    return { 'model': best_model, 'hyperparameters': best_hps, 'tuner': tuner, 'val_acc': best_model.evaluate(X_val_tune, y_val_encoded, verbose=0)[1] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run separate hyperparameter tuning for each model type\n",
    "tuning_results = {\n",
    "    model_type: run_model_specific_tuning(model_type, max_trials=30) for model_type in ('lstm', 'mlp', 'cnn', )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36565285",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type, results in tuning_results.items():\n",
    "    print(\"\\nBest %s hyperparameters:\" % model_type.upper())\n",
    "    print(\"=\"*50)\n",
    "    for param_value in results[\"hyperparameters\"].values.items():\n",
    "        print(\"%s: %s\" % param_value)\n",
    "    print(\"Best %s validation accuracy: %.4f\" % (model_type.upper(), results[\"val_acc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa28817",
   "metadata": {},
   "source": [
    "## 8. Results and Analysis\n",
    "**Evaluation Metrics**:\n",
    "- Accuracy: Overall classification performance\n",
    "- Precision, Recall, F1-score: Per-class performance metrics\n",
    "- Confusion Matrix: Visual representation of classification patterns\n",
    "- Cross-validation: Robust performance estimation across different data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and compare the best models from each architecture type\n",
    "model_histories = {}\n",
    "trained_models = {}\n",
    "\n",
    "for model_type, results in tuning_results.items():\n",
    "    print(\"\\n--- Training Best %s Model ---\" % model_type.upper())\n",
    "\n",
    "    # Assign the appropriate model and data\n",
    "    if model_type == 'mlp':\n",
    "        train_data, val_data = X_train_reshaped, X_val_reshaped\n",
    "        model = build_mlp_model(results[\"hyperparameters\"])\n",
    "    elif model_type == 'cnn':\n",
    "        train_data, val_data = X_train_reshaped, X_val_reshaped\n",
    "        model = build_cnn_model(results[\"hyperparameters\"])\n",
    "    elif model_type == 'lstm':\n",
    "        train_data, val_data = X_train_seq_padded, X_val_seq_padded\n",
    "        model = build_lstm_model(results[\"hyperparameters\"])\n",
    "\n",
    "    # Training again to obtain history\n",
    "    history = model.fit(\n",
    "        train_data, y_train_encoded,\n",
    "        validation_data=(val_data, y_val_encoded),\n",
    "        epochs=80,\n",
    "        batch_size=32,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=12, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(patience=6, factor=.5, min_lr=1e-7)\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Store the trained model and history\n",
    "    trained_models[model_type] = model\n",
    "    model_histories[model_type] = history\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_accuracy = model.evaluate(val_data, y_val_encoded, verbose=0)[1]\n",
    "    print(\"Best %s validation accuracy: %.4f\" % (model_type.upper(), val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af258d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training histories across model types\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(2, 2, 1)\n",
    "for model_type, history in model_histories.items():\n",
    "    plt.plot(history.history['accuracy'], label=f'{model_type.upper()} Training', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "for model_type, history in model_histories.items():\n",
    "    plt.plot(history.history['val_accuracy'], label=f'{model_type.upper()} Validation', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(2, 2, 3)\n",
    "for model_type, history in model_histories.items():\n",
    "    plt.plot(history.history['loss'], label=f'{model_type.upper()} Training', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot validation loss\n",
    "plt.subplot(2, 2, 4)\n",
    "for model_type, history in model_histories.items():\n",
    "    plt.plot(history.history['val_loss'], label=f'{model_type.upper()} Validation', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set for all best model types\n",
    "print(\"=== FINAL EVALUATION ON TEST SET (FOLD 10) - ALL MODEL TYPES ===\")\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "for model_type, model in trained_models.items():\n",
    "    print(f\"\\n--- Evaluating Best {model_type.upper()} Model ---\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_seq_padded if model_type==\"lstm\" else X_test_reshaped, y_test_encoded, verbose=0)\n",
    "    y_pred = model.predict(X_test_seq_padded if model_type==\"lstm\" else X_test_reshaped, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Store results\n",
    "    test_results[model_type] = {\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_loss': test_loss,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_classes': y_pred_classes\n",
    "    }\n",
    "\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Classification report\n",
    "    print(f\"\\nClassification Report for {model_type.upper()}:\")\n",
    "    print(classification_report(y_test_encoded, y_pred_classes,\n",
    "                              target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d2880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative performance analysis\n",
    "comparison_data = []\n",
    "for model_type, results in test_results.items():\n",
    "    # Calculate additional metrics\n",
    "    cm = confusion_matrix(y_test_encoded, results['y_pred_classes'])\n",
    "    precision = np.diag(cm) / np.sum(cm, axis=0)\n",
    "    recall = np.diag(cm) / np.sum(cm, axis=1)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    comparison_data.append({\n",
    "        'Model Type': model_type.upper(),\n",
    "        'Test Accuracy': results['test_accuracy'],\n",
    "        'Test Loss': results['test_loss'],\n",
    "        'Mean Precision': np.nanmean(precision),\n",
    "        'Mean Recall': np.nanmean(recall),\n",
    "        'Mean F1-Score': np.nanmean(f1),\n",
    "        'Hyperparameter Score': tuning_results[model_type]['val_acc']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"Performance Comparison Across Model Types:\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of model performances\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Test accuracy comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "model_types = comparison_df['Model Type']\n",
    "test_accuracies = comparison_df['Test Accuracy']\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "bars = plt.bar(model_types, test_accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Test Accuracy Comparison')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, accuracy in zip(bars, test_accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{accuracy:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# F1-score comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "f1_scores = comparison_df['Mean F1-Score']\n",
    "bars = plt.bar(model_types, f1_scores, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.ylabel('Mean F1-Score')\n",
    "plt.title('F1-Score Comparison')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, f1_score in zip(bars, f1_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{f1_score:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Training time and complexity analysis (approximate)\n",
    "plt.subplot(1, 3, 3)\n",
    "# Count parameters as proxy for model complexity\n",
    "param_counts = {}\n",
    "for model_type, model in trained_models.items():\n",
    "    param_counts[model_type] = model.count_params() / 1e6  # In millions\n",
    "\n",
    "param_values = [param_counts.get(mt.lower(), 0) for mt in model_types]\n",
    "bars = plt.bar(model_types, param_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.ylabel('Parameters (Millions)')\n",
    "plt.title('Model Complexity (Parameter Count)')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, params in zip(bars, param_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{params:.2f}M', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08798163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best overall model based on test performance\n",
    "best_overall_type = comparison_df.iloc[0]['Model Type'].lower()\n",
    "best_overall_model = trained_models[best_overall_type]\n",
    "best_overall_results = test_results[best_overall_type]\n",
    "\n",
    "print(\"\\nBest overall model:\", best_overall_type.upper())\n",
    "print(\"Test Accuracy: %.4f\" % best_overall_results['test_accuracy'])\n",
    "\n",
    "# Update the final_model variable to use the best overall model\n",
    "final_model = best_overall_model\n",
    "final_history = model_histories[best_overall_type]\n",
    "\n",
    "# Get the best configuration from tuning results\n",
    "best_hps = tuning_results[best_overall_type]['hyperparameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67235235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Fold Cross-Validation with Detailed Metrics and Confusion Matrix\n",
    "def run_comprehensive_cross_validation(all_features, all_features_sequential, all_labels, all_folds, best_hps, n_folds=10):\n",
    "    \"\"\"Comprehensive cross-validation with confusion matrix aggregation\"\"\"\n",
    "\n",
    "    fold_accuracies = []\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "    label_encoder_cv = LabelEncoder()\n",
    "    labels_encoded = label_encoder_cv.fit_transform(all_labels)\n",
    "\n",
    "    for test_fold in range(1, n_folds + 1):\n",
    "        print(f\"\\n--- Fold {test_fold}/{n_folds} ---\")\n",
    "\n",
    "        # Use current fold for testing, previous fold for validation\n",
    "        val_fold = test_fold - 1 if test_fold > 1 else n_folds\n",
    "        train_folds = [f for f in range(1, n_folds + 1) if f not in [test_fold, val_fold]]\n",
    "\n",
    "        # Get indices using precomputed features\n",
    "        train_mask = np.isin(all_folds, train_folds)\n",
    "        val_mask = all_folds == val_fold\n",
    "        test_mask = all_folds == test_fold\n",
    "        \n",
    "        # For MLP/CNN (aggregated features)\n",
    "        y_train_fold = labels_encoded[train_mask]\n",
    "        y_val_fold = labels_encoded[val_mask]\n",
    "        y_test_fold = labels_encoded[test_mask]\n",
    "\n",
    "        # Rebuild and train model for this fold\n",
    "        if best_overall_type == 'lstm':\n",
    "            model_fold = build_lstm_model(best_hps)\n",
    "\n",
    "            # For LSTM (sequential features)\n",
    "            X_train_seq_fold = [all_features_sequential[i] for i in range(len(all_features_sequential)) if train_mask[i]]\n",
    "            X_val_seq_fold = [all_features_sequential[i] for i in range(len(all_features_sequential)) if val_mask[i]]\n",
    "            X_test_seq_fold = [all_features_sequential[i] for i in range(len(all_features_sequential)) if test_mask[i]]\n",
    "\n",
    "            # Scale and pad sequential features for LSTM\n",
    "            all_train_seq_fold = np.vstack(X_train_seq_fold)\n",
    "            scaler_seq_fold = StandardScaler()\n",
    "            scaler_seq_fold.fit(all_train_seq_fold)\n",
    "\n",
    "            X_train_seq_scaled_fold = [scaler_seq_fold.transform(seq) for seq in X_train_seq_fold]\n",
    "            X_val_seq_scaled_fold = [scaler_seq_fold.transform(seq) for seq in X_val_seq_fold]\n",
    "            X_test_seq_scaled_fold = [scaler_seq_fold.transform(seq) for seq in X_test_seq_fold]\n",
    "\n",
    "            # Pad sequences to consistent length\n",
    "            max_length_fold = max(len(seq) for seq in X_train_seq_scaled_fold)\n",
    "            X_train_seq_padded_fold = pad_sequences(X_train_seq_scaled_fold, maxlen=max_length_fold, dtype='float32', padding='post')\n",
    "            X_val_seq_padded_fold = pad_sequences(X_val_seq_scaled_fold, maxlen=max_length_fold, dtype='float32', padding='post')\n",
    "            X_test_seq_padded_fold = pad_sequences(X_test_seq_scaled_fold, maxlen=max_length_fold, dtype='float32', padding='post')\n",
    "\n",
    "            train_data, val_data, test_data = X_train_seq_padded_fold, X_val_seq_padded_fold, X_test_seq_padded_fold\n",
    "        else:\n",
    "            # For MLP/CNN (aggregated features)\n",
    "            X_train_fold = all_features[train_mask]\n",
    "            X_val_fold = all_features[val_mask]\n",
    "            X_test_fold = all_features[test_mask]\n",
    "\n",
    "            # Scale features per fold\n",
    "            scaler_fold = StandardScaler()\n",
    "            X_train_scaled_fold = scaler_fold.fit_transform(X_train_fold)\n",
    "            X_val_scaled_fold = scaler_fold.transform(X_val_fold)\n",
    "            X_test_scaled_fold = scaler_fold.transform(X_test_fold)\n",
    "\n",
    "            # Reshape for CNN/MLP\n",
    "            X_train_reshaped_fold = X_train_scaled_fold.reshape(X_train_scaled_fold.shape[0], X_train_scaled_fold.shape[1], 1)\n",
    "            X_val_reshaped_fold = X_val_scaled_fold.reshape(X_val_scaled_fold.shape[0], X_val_scaled_fold.shape[1], 1)\n",
    "            X_test_reshaped_fold = X_test_scaled_fold.reshape(X_test_scaled_fold.shape[0], X_test_scaled_fold.shape[1], 1)\n",
    "\n",
    "            train_data, val_data, test_data = X_train_reshaped_fold, X_val_reshaped_fold, X_test_reshaped_fold\n",
    "            if best_overall_type == 'mlp':\n",
    "                model_fold = build_mlp_model(best_hps)\n",
    "            elif best_overall_type == 'cnn':\n",
    "                model_fold = build_cnn_model(best_hps)\n",
    "\n",
    "        model_fold.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=best_hps['learning_rate']),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        history_fold = model_fold.fit(\n",
    "            train_data, y_train_fold,\n",
    "            validation_data=(val_data, y_val_fold),\n",
    "            epochs=80,\n",
    "            batch_size=32,\n",
    "            verbose=0,\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Evaluate on test fold\n",
    "        test_accuracy_fold = model_fold.evaluate(test_data, y_test_fold, verbose=0)[1]\n",
    "        fold_accuracies.append(test_accuracy_fold)\n",
    "\n",
    "        # Get predictions for confusion matrix\n",
    "        y_pred_fold = model_fold.predict(test_data, verbose=0)\n",
    "        y_pred_classes_fold = np.argmax(y_pred_fold, axis=1)\n",
    "\n",
    "        # Store true and predicted labels for overall confusion matrix\n",
    "        all_true_labels.extend(y_test_fold)\n",
    "        all_predicted_labels.extend(y_pred_classes_fold)\n",
    "\n",
    "        print(f\"Fold {test_fold} Test Accuracy: {test_accuracy_fold:.4f}\")\n",
    "        print(f\"Fold {test_fold} Samples: {len(y_test_fold)}\")\n",
    "\n",
    "        # Clean up\n",
    "        del model_fold\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    return fold_accuracies, all_true_labels, all_predicted_labels\n",
    "\n",
    "# Run comprehensive cross-validation\n",
    "cv_accuracies, cv_true_labels, cv_predicted_labels = run_comprehensive_cross_validation(\n",
    "    all_features, all_features_sequential, all_labels, all_folds, best_hps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e6d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overall confusion matrix for 10-fold cross-validation\n",
    "print(\"\\n=== 10-FOLD CROSS-VALIDATION CONFUSION MATRIX ===\")\n",
    "\n",
    "# Convert back to original class names\n",
    "cv_true_labels_names = label_encoder.inverse_transform(cv_true_labels)\n",
    "cv_predicted_labels_names = label_encoder.inverse_transform(cv_predicted_labels)\n",
    "\n",
    "# Create the overall confusion matrix\n",
    "cv_cm = confusion_matrix(cv_true_labels_names, cv_predicted_labels_names, labels=label_encoder.classes_)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cv_cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_,\n",
    "            cbar_kws={'label': 'Number of Samples'})\n",
    "plt.title('Overall Confusion Matrix - 10-Fold Cross-Validation\\n'\n",
    "          f'Mean Accuracy: {np.mean(cv_accuracies):.4f} (Â±{np.std(cv_accuracies):.4f})')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c964a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix (by true labels)\n",
    "cv_cm_normalized = cv_cm.astype('float') / cv_cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cv_cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_,\n",
    "            cbar_kws={'label': 'Proportion'})\n",
    "plt.title('Normalized Confusion Matrix - 10-Fold Cross-Validation\\n'\n",
    "          '(Normalized by True Label)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b408011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class performance analysis from cross-validation\n",
    "cv_class_report = classification_report(cv_true_labels_names, cv_predicted_labels_names, target_names=label_encoder.classes_, output_dict=True)\n",
    "cv_class_report_df = pd.DataFrame(cv_class_report).transpose()\n",
    "\n",
    "print(\"Classification Report (10-Fold CV):\")\n",
    "print(classification_report(cv_true_labels_names, cv_predicted_labels_names, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ec50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single test fold vs cross-validation performance\n",
    "\n",
    "# Single fold confusion matrix (from previous evaluation)\n",
    "single_cm = confusion_matrix(y_test_encoded, y_pred_classes)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Single fold confusion matrix\n",
    "single_cm_normalized = single_cm.astype('float') / single_cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(single_cm_normalized, annot=True, fmt='.2f', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "axes[0].set_title(f'Single Test Fold (Fold 10) (Normalized by True Label)\\nAccuracy: {test_accuracy:.4f}')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].tick_params(axis='y', rotation=0)\n",
    "\n",
    "# Cross-validation confusion matrix\n",
    "sns.heatmap(cv_cm_normalized, annot=True, fmt='.2f', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "axes[1].set_title(f'10-Fold Cross-Validation (Normalized by True Label)\\nMean Accuracy: {np.mean(cv_accuracies):.4f}')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].tick_params(axis='y', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f407c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of cross-validation results\n",
    "\n",
    "# Calculate per-class accuracy from confusion matrix\n",
    "class_accuracies = np.diag(cv_cm) / np.sum(cv_cm, axis=1)\n",
    "\n",
    "# Create detailed performance summary\n",
    "cv_performance_df = pd.DataFrame({\n",
    "    'Class': label_encoder.classes_,\n",
    "    'Samples': np.sum(cv_cm, axis=1),\n",
    "    'Correct_Predictions': np.diag(cv_cm),\n",
    "    'Class_Accuracy': class_accuracies,\n",
    "    'Precision': cv_class_report_df.loc[label_encoder.classes_, 'precision'].values,\n",
    "    'Recall': cv_class_report_df.loc[label_encoder.classes_, 'recall'].values,\n",
    "    'F1_Score': cv_class_report_df.loc[label_encoder.classes_, 'f1-score'].values\n",
    "})\n",
    "\n",
    "print(\"\\nPer-Class Performance (10-Fold Cross-Validation):\")\n",
    "display(cv_performance_df)\n",
    "\n",
    "# Identify best and worst performing classes\n",
    "best_classes = cv_performance_df.nlargest(3, 'Class_Accuracy')\n",
    "worst_classes = cv_performance_df.nsmallest(3, 'Class_Accuracy')\n",
    "\n",
    "print(f\"\\nBest Performing Classes:\")\n",
    "for _, row in best_classes.iterrows():\n",
    "    print(f\"  {row['Class']}: {row['Class_Accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nMost Challenging Classes:\")\n",
    "for _, row in worst_classes.iterrows():\n",
    "    print(f\"  {row['Class']}: {row['Class_Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class-wise performance\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Class accuracy comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "sorted_indices = np.argsort(class_accuracies)[::-1]\n",
    "sorted_classes = [label_encoder.classes_[i] for i in sorted_indices]\n",
    "sorted_accuracies = class_accuracies[sorted_indices]\n",
    "\n",
    "bars = plt.bar(range(len(sorted_classes)), sorted_accuracies, color='lightblue', alpha=0.7)\n",
    "plt.xticks(range(len(sorted_classes)), sorted_classes, rotation=45)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Class-wise Accuracy (10-Fold CV)')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, sorted_accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Precision-Recall comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "x = range(len(label_encoder.classes_))\n",
    "width = 0.35\n",
    "plt.bar([i - width/2 for i in x], cv_performance_df['Precision'], width,\n",
    "        label='Precision', alpha=0.7)\n",
    "plt.bar([i + width/2 for i in x], cv_performance_df['Recall'], width,\n",
    "        label='Recall', alpha=0.7)\n",
    "plt.xticks(x, label_encoder.classes_, rotation=45)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision vs Recall by Class')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# F1-Score distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(range(len(sorted_classes)), cv_performance_df.sort_values('F1_Score', ascending=False)['F1_Score'],\n",
    "        color='lightgreen', alpha=0.7)\n",
    "plt.xticks(range(len(sorted_classes)),\n",
    "           cv_performance_df.sort_values('F1_Score', ascending=False)['Class'],\n",
    "           rotation=45)\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('F1-Score by Class (Sorted)')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d71dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning Analysis\n",
    "print(\"Best Model Type:\", best_overall_type)\n",
    "print(\"Best Model Configuration:\")\n",
    "for param_value in best_hps.values.items():\n",
    "    print(\"\\t%s: %s\" % param_value)\n",
    "\n",
    "# Visualize hyperparameter search results\n",
    "tuner_data = []\n",
    "for model_type, results in tuning_results.items():\n",
    "    top_trials = results[\"tuner\"].oracle.get_best_trials(num_trials=3)\n",
    "    for trial in top_trials:\n",
    "        tuner_data.append({'model_type': model_type, 'score': trial.score, **trial.hyperparameters.values})\n",
    "\n",
    "tuner_df = pd.DataFrame(tuner_data)\n",
    "print(\"\\nTop 3 Hyperparameter Configurations from each model type:\")\n",
    "display(tuner_df.sort_values('score', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary Table\n",
    "pd.DataFrame({\n",
    "    'Evaluation Method': ['Single Test Fold (Fold 10)', '10-Fold Cross-Validation'],\n",
    "    'Accuracy': ['%.4f Â± %.4f' % (test_accuracy, 0.), '%.4f Â± %.4f' % (np.mean(cv_accuracies), np.std(cv_accuracies))],\n",
    "    'Best Model': [best_overall_type, best_overall_type],\n",
    "    'Learning Rate': [best_hps['learning_rate'], best_hps['learning_rate']]\n",
    "}).style.set_caption(\"Final performance summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89173bb",
   "metadata": {},
   "source": [
    "## 9. Discussion and Conclusion\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "- Preprocessing failed on some of the much shorter clips. The issue was fixed by setting `mode='nearest'` to the two `librosa.feature.delta()` calls.\n",
    "- Feature extraction took too much time especially during the final training, where the designation of training, validation, and testing data was constantly changing. This was fixed by extracting features from each fold separately at the very beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212b751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
