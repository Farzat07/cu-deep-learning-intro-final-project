{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c4ce1f",
   "metadata": {},
   "source": [
    "# Urban Sound Classification using Deep Learning\n",
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "**Type of Learning**: Supervised Deep Learning\n",
    "\n",
    "**Algorithms**: Deep Neural Networks (MLP, CNN, LSTM) with Hyperparameter Tuning\n",
    "\n",
    "**Task**: Multi-class Classification of Urban Sounds\n",
    "\n",
    "This project focuses on classifying urban sounds into 10 different categories using deep learning approaches. The UrbanSound8K dataset contains 8732 labeled sound excerpts from urban environments, which we use to train and evaluate various neural network architectures.\n",
    "\n",
    "## 2. Motivation and Goal\n",
    "\n",
    "**Motivation**: Urban sound classification has important applications in:\n",
    "- Smart city monitoring and noise pollution analysis\n",
    "- Audio surveillance systems\n",
    "- Environmental sound recognition for IoT devices\n",
    "- Audio-based context awareness in mobile applications\n",
    "\n",
    "**Goal**: Develop an accurate and robust deep learning model that can classify urban sounds into 10 distinct categories with high accuracy, using advanced hyperparameter tuning and proper evaluation methodologies.\n",
    "\n",
    "## 3. Dataset Source and Citation\n",
    "\n",
    "**Dataset**: UrbanSound8K Dataset\n",
    "\n",
    "**Source**: https://urbansounddataset.weebly.com/urbansound8k.html\n",
    "\n",
    "**Citation**:\n",
    "J. Salamon, C. Jacoby and J. P. Bello, \"A Dataset and Taxonomy for Urban Sound Research\",\n",
    "22nd ACM International Conference on Multimedia, Orlando SA, Nov. 2014.\n",
    "\n",
    "Dataset compiled by Justin Salamon, Christopher Jacoby and Juan Pablo Bello. All files are excerpts of recordings\n",
    "uploaded to www.freesound.org. Please see FREESOUNDCREDITS.txt for an attribution list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a0894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "DATASET_PATH = \"UrbanSound8K\"\n",
    "METADATA_FILE = os.path.join(DATASET_PATH, \"metadata\", \"UrbanSound8K.csv\")\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(METADATA_FILE)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Dataset shape:\", metadata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c25d6f",
   "metadata": {},
   "source": [
    "## 4. Dataset Description\n",
    "**Data Size**:\n",
    "- Total sound files: 8,732 excerpts\n",
    "- Classes: 10 urban sound categories\n",
    "- Typical length: <= 4 seconds duration\n",
    "- Sampling: Original files vary in length, excerpts are up to 4 seconds\n",
    "- Format: WAV files organized in 10 folds for cross-validation\n",
    "\n",
    "The sampling rate, bit depth, and number of channels are the same as those of the original file uploaded to Freesound (and hence may vary from file to file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12289f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic dataset information\n",
    "print(\"Total number of audio files:\", len(metadata))\n",
    "print(\"Number of unique classes:\", metadata['class'].nunique())\n",
    "print(\"Classes:\", sorted(metadata['class'].unique()))\n",
    "\n",
    "# Display dataset structure\n",
    "display(metadata.head())\n",
    "\n",
    "# Display dataset columns\n",
    "print(\"Dataset columns:\", metadata.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73379f43",
   "metadata": {},
   "source": [
    "## 5. EXPLORATORY DATA ANALYSIS (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f3e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Label Distribution Analysis\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Class distribution\n",
    "plt.subplot(2, 2, 1)\n",
    "class_counts = metadata['class'].value_counts()\n",
    "sns.barplot(x=class_counts.values, y=class_counts.index, palette='viridis')\n",
    "plt.title('Distribution of Sound Classes')\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.ylabel('Class')\n",
    "\n",
    "# Fold distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "fold_counts = metadata['fold'].value_counts().sort_index()\n",
    "sns.barplot(x=fold_counts.index, y=fold_counts.values, palette='coolwarm')\n",
    "plt.title('Distribution of Samples Across Folds')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Number of Samples')\n",
    "\n",
    "# Duration statistics\n",
    "plt.subplot(2, 2, 3)\n",
    "durations_by_class = []\n",
    "class_names = []\n",
    "for class_name in sorted(metadata['class'].unique()):\n",
    "    class_durations = metadata[metadata['class'] == class_name]['end'] - metadata[metadata['class'] == class_name]['start']\n",
    "    durations_by_class.append(class_durations)\n",
    "    class_names.append(class_name)\n",
    "\n",
    "plt.boxplot(durations_by_class, labels=class_names)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Duration Distribution by Class (Box Plot)')\n",
    "plt.ylabel('Duration (seconds)')\n",
    "plt.grid(True, alpha=.3)\n",
    "\n",
    "# Sample audio file analysis\n",
    "plt.subplot(2, 2, 4)\n",
    "# Analyze a few sample files to show waveform diversity\n",
    "sample_files = metadata.sample(1, random_state=42)  # Just one sample for this small subplot\n",
    "for _, row in sample_files.iterrows():\n",
    "    file_path = os.path.join(DATASET_PATH, \"audio\", f\"fold{row['fold']}\", row['slice_file_name'])\n",
    "    audio, sr = librosa.load(file_path, sr=22050)\n",
    "    time_axis = np.linspace(0, len(audio)/sr, len(audio))\n",
    "    plt.plot(time_axis, audio, color='green', alpha=.7)\n",
    "    plt.title('Sample Waveform: %s' % row[\"class\"])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True, alpha=.3)\n",
    "    break  # Just plot one\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad7c09",
   "metadata": {},
   "source": [
    "### 5.1 Observations\n",
    "\n",
    "Most classes seem to have around 1000 samples a few with a significantly lower number of samples. Even then, this does not warrant resampling as the overall distribution is mostly balanced.\n",
    "\n",
    "Another important observation is that the folds do not contain the exact same number of samples. In fact, even if each contained the same number of samples, the *amount of data* would not be the same as the duration varies between samples. An important implication is thatfor a fair comparison, 10-fold cross-validation is necessary.\n",
    "\n",
    "It is also interesting to note that except for car horns and gun shots, all clips have a median of around 4.0. Gun shots have a median way below. While it is not used here, I suspect that including the duration as a feature/parameter while training would improve the scores of these two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d92cc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 5.2 Audio Characteristics Analysis\n",
    "\n",
    "# Analyze a few sample files to demonstrate audio properties\n",
    "sample_analysis = []\n",
    "for idx, row in metadata.sample(20, random_state=42).iterrows():\n",
    "    file_path = os.path.join(DATASET_PATH, \"audio\", f\"fold{row['fold']}\", row['slice_file_name'])\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    duration = len(audio) / sr\n",
    "    sample_analysis.append({\n",
    "        'class': row['class'],\n",
    "        'duration': duration,\n",
    "        'sample_rate': sr,\n",
    "        'samples': len(audio),\n",
    "        'max_amplitude': np.max(np.abs(audio))\n",
    "    })\n",
    "\n",
    "pd.DataFrame(sample_analysis).style.set_caption(\"Audio characteristics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6ee772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Visualize Sample Audio Files\n",
    "def plot_sample_audios(metadata, n_samples=3):\n",
    "    \"\"\"Plot waveform and spectrogram for sample audio files\"\"\"\n",
    "    fig, axes = plt.subplots(n_samples, 3, figsize=(15, 4*n_samples))\n",
    "\n",
    "    sample_data = metadata.sample(n_samples, random_state=25)\n",
    "\n",
    "    for idx, (_, row) in enumerate(sample_data.iterrows()):\n",
    "        file_path = os.path.join(DATASET_PATH, \"audio\", f\"fold{row['fold']}\", row['slice_file_name'])\n",
    "\n",
    "        audio, sr = librosa.load(file_path, sr=22050)\n",
    "\n",
    "        # Waveform\n",
    "        axes[idx, 0].plot(np.linspace(0, len(audio)/sr, len(audio)), audio)\n",
    "        axes[idx, 0].set_title(\"Waveform: %s\" % row['class'])\n",
    "        axes[idx, 0].set_xlabel('Time (s)')\n",
    "        axes[idx, 0].set_ylabel('Amplitude')\n",
    "\n",
    "        # Spectrogram\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "        img = librosa.display.specshow(D, y_axis='log', x_axis='time', sr=sr, ax=axes[idx, 1])\n",
    "        axes[idx, 1].set_title(\"Spectrogram: %s\" % row['class'])\n",
    "        plt.colorbar(img, ax=axes[idx, 1])\n",
    "\n",
    "        # MFCCs\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "        librosa.display.specshow(mfccs, x_axis='time', ax=axes[idx, 2])\n",
    "        axes[idx, 2].set_title(\"MFCCs: %s\" % row['class'])\n",
    "        plt.colorbar(img, ax=axes[idx, 2])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot sample audio analysis\n",
    "plot_sample_audios(metadata, n_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Duration Analysis\n",
    "durations = metadata['end'] - metadata['start']\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(durations, bins=30, alpha=.7, color='lightblue', edgecolor='black')\n",
    "plt.title('Distribution of Audio Durations')\n",
    "plt.xlabel('Duration (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=durations)\n",
    "plt.title('Box Plot of Audio Durations')\n",
    "plt.xlabel('Duration (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Duration Statistics:\")\n",
    "print(\"Mean: %.2fs\" % durations.mean())\n",
    "print(\"Std: %.2fs\" % durations.std())\n",
    "print(\"Min: %.2fs\" % durations.min())\n",
    "print(\"Max: %.2fs\" % durations.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def5d0d",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing\n",
    "**Preprocessing Steps**:\n",
    "1. **Feature Extraction**: MFCC (Mel-Frequency Cepstral Coefficients) with delta and delta-delta features\n",
    "2. **Feature Aggregation**: Mean, standard deviation, and median across time frames\n",
    "3. **Feature Scaling**: StandardScaler for normalization\n",
    "4. **Label Encoding**: Convert class labels to numerical format\n",
    "5. **Data Splitting**: Predefined folds for training, validation, and testing\n",
    "\n",
    "**Benefits of the preprocessing steps**:\n",
    "- MFCCs are well-established for audio classification as they capture perceptual frequency characteristics\n",
    "- Delta features capture temporal dynamics of the audio signal\n",
    "- Statistical aggregation reduces variable-length audio to fixed-length feature vectors\n",
    "- Standard scaling ensures stable training of neural networks\n",
    "- Fixed fold assignment ensures reproducible evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22621736",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define fixed folds for model selection\n",
    "VAL_FOLD = 9\n",
    "TEST_FOLD = 10\n",
    "TRAIN_FOLDS = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "print(\"Fold Assignment:\")\n",
    "print(\"Training folds:\", TRAIN_FOLDS)\n",
    "print(\"Validation fold:\", VAL_FOLD)\n",
    "print(\"Test fold:\", TEST_FOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e907036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute features for all data once\n",
    "def precompute_all_features(metadata):\n",
    "    \"\"\"Precompute features for all audio files efficiently\"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_folds = []\n",
    "\n",
    "    print(\"Precomputing features for all audio files...\")\n",
    "    for idx, row in metadata.iterrows():\n",
    "        file_path = os.path.join(DATASET_PATH, \"audio\", f\"fold{row['fold']}\", row['slice_file_name'])\n",
    "\n",
    "        audio, sr = librosa.load(file_path, sr=22050)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40, n_fft=2048, hop_length=512)\n",
    "\n",
    "        # Must set mode='nearest' in order to handle shorter clips\n",
    "        mfccs_delta = librosa.feature.delta(mfccs, mode='nearest')\n",
    "        mfccs_delta2 = librosa.feature.delta(mfccs, order=2, mode='nearest')\n",
    "\n",
    "        features_combined = np.vstack([mfccs, mfccs_delta, mfccs_delta2])\n",
    "        features_aggregated = np.concatenate([\n",
    "            np.mean(features_combined, axis=1),\n",
    "            np.std(features_combined, axis=1),\n",
    "            np.median(features_combined, axis=1)\n",
    "        ])\n",
    "\n",
    "        all_features.append(features_aggregated)\n",
    "        all_labels.append(row['class'])\n",
    "        all_folds.append(row['fold'])\n",
    "\n",
    "    return np.array(all_features), np.array(all_labels), np.array(all_folds)\n",
    "\n",
    "# Precompute features for entire dataset\n",
    "all_features, all_labels, all_folds = precompute_all_features(metadata)\n",
    "print(\"Precomputed features shape:\", all_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898135c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data using precomputed features\n",
    "train_mask = np.isin(all_folds, TRAIN_FOLDS)\n",
    "val_mask = all_folds == VAL_FOLD\n",
    "test_mask = all_folds == TEST_FOLD\n",
    "\n",
    "X_train, y_train = all_features[train_mask], all_labels[train_mask]\n",
    "X_val, y_val = all_features[val_mask], all_labels[val_mask]\n",
    "X_test, y_test = all_features[test_mask], all_labels[test_mask]\n",
    "\n",
    "print(\"Training set:\", X_train.shape[0], \"samples\")\n",
    "print(\"Validation set:\", X_val.shape[0], \"samples\")\n",
    "print(\"Test set:\", X_test.shape[0], \"samples\")\n",
    "\n",
    "# Verify class distribution\n",
    "print(\"\\nClass distribution across splits:\")\n",
    "print(\"\\nTraining:\", pd.Series(y_train).value_counts().sort_index(), sep='\\n')\n",
    "print(\"\\nValidation:\", pd.Series(y_val).value_counts().sort_index(), sep='\\n')\n",
    "print(\"\\nTest:\", pd.Series(y_test).value_counts().sort_index(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e08e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(\"Class mapping:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    print(\"%s: %d\" % (class_name, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4eacf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for CNN/LSTM models\n",
    "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "X_val_reshaped = X_val_scaled.reshape(X_val_scaled.shape[0], X_val_scaled.shape[1], 1)\n",
    "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c09434b",
   "metadata": {},
   "source": [
    "## 7. Model Selection and Architecture\n",
    "**Choice of Models**:\n",
    "1. **MLP (Multi-Layer Perceptron)**: Baseline model for comparison, good for structured feature data\n",
    "2. **CNN (Convolutional Neural Network)**: Effective for capturing local patterns in feature sequences\n",
    "3. **LSTM (Long Short-Term Memory)**: Suitable for temporal sequence modeling in audio data\n",
    "\n",
    "**Rationale**:\n",
    "- **MLP**: Simple baseline to establish performance benchmark\n",
    "- **CNN**: Can learn hierarchical features from MFCC sequences\n",
    "- **LSTM**: Can model temporal dependencies in audio signals\n",
    "- **Hyperparameter Tuning**: Bayesian optimization to find optimal architecture and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131fd09a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ADVANCED HYPERPARAMETER TUNING WITH KERAS TUNER\n",
    "def build_model(hp):\n",
    "    \"\"\"Build model with tunable hyperparameters using Keras Tuner\"\"\"\n",
    "\n",
    "    model_type = hp.Choice('model_type', ['mlp', 'cnn', 'lstm'])\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    # Tunable learning rate\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "\n",
    "    if model_type == 'mlp':\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Input(shape=(input_dim,)))\n",
    "\n",
    "        # Tunable number of layers\n",
    "        for i in range(hp.Int('num_layers', 2, 4)):\n",
    "            units = hp.Int(f'units_{i}', min_value=128, max_value=512, step=128)\n",
    "\n",
    "            model.add(layers.Dense(units, activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(hp.Float(f'dropout_{i}', .2, .6)))\n",
    "\n",
    "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    elif model_type == 'cnn':\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Reshape((input_dim, 1), input_shape=(input_dim,)))\n",
    "\n",
    "        # Tunable CNN architecture\n",
    "        for i in range(hp.Int('conv_layers', 2, 4)):\n",
    "            filters = hp.Int(f'filters_{i}', min_value=32, max_value=256, step=32)\n",
    "            kernel_size = hp.Int(f'kernel_{i}', min_value=3, max_value=7, step=2)\n",
    "\n",
    "            model.add(layers.Conv1D(filters, kernel_size, activation='relu', padding='same'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "        model.add(layers.GlobalAveragePooling1D())\n",
    "\n",
    "        # Tunable dense layers\n",
    "        for i in range(hp.Int('dense_layers', 1, 3)):\n",
    "            units = hp.Int(f'dense_units_{i}', min_value=64, max_value=256, step=64)\n",
    "            model.add(layers.Dense(units, activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(hp.Float('dense_dropout', .3, .6)))\n",
    "\n",
    "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    elif model_type == 'lstm':\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Reshape((input_dim, 1), input_shape=(input_dim,)))\n",
    "\n",
    "        # Bidirectional LSTM layers\n",
    "        for i in range(hp.Int('lstm_layers', 1, 3)):\n",
    "            units = hp.Int(f'lstm_units_{i}', min_value=32, max_value=128, step=32)\n",
    "            return_sequences = i < hp.Int('lstm_layers', 1, 3) - 1  # Last layer doesn't return sequences\n",
    "\n",
    "            model.add(layers.Bidirectional(layers.LSTM(units, return_sequences=return_sequences)))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(hp.Float(f'lstm_dropout_{i}', .2, .5)))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Tunable optimizer\n",
    "    optimizer_name = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e61e8c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_advanced_hyperparameter_tuning(X_train, y_train, X_val, y_val, max_trials=25):\n",
    "    \"\"\"Run advanced hyperparameter tuning with Bayesian optimization\"\"\"\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        build_model,\n",
    "        objective='val_accuracy',\n",
    "        max_trials=max_trials,\n",
    "        executions_per_trial=1,  # Run each trial once for speed (can increase to 2 for stability)\n",
    "        directory='advanced_tuning',\n",
    "        project_name='urban_sound_advanced',\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    print(\"Starting advanced hyperparameter tuning with Bayesian Optimization...\")\n",
    "    print(\"Max trials:\", max_trials)\n",
    "    print(\"Training samples:\", X_train.shape[0])\n",
    "    print(\"Validation samples:\", X_val.shape[0])\n",
    "\n",
    "    # Callbacks for training\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, min_lr=1e-7)\n",
    "    ]\n",
    "\n",
    "    # Perform the search\n",
    "    tuner.search(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Get best hyperparameters and model\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BEST HYPERPARAMETERS FOUND:\")\n",
    "    print(\"=\"*60)\n",
    "    for param, value in best_hps.values.items():\n",
    "        print(\"%s: %s\" % (param, value))\n",
    "\n",
    "    # Evaluate best model on validation set\n",
    "    val_accuracy = best_model.evaluate(X_val, y_val, verbose=0)[1]\n",
    "    print(\"Best model validation accuracy: %.4f\" % val_accuracy)\n",
    "\n",
    "    return best_model, best_hps, tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6311f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run advanced hyperparameter tuning\n",
    "print(\"=== ADVANCED HYPERPARAMETER TUNING WITH KERAS TUNER ===\")\n",
    "\n",
    "best_model_advanced, best_hps, tuner = run_advanced_hyperparameter_tuning(\n",
    "    X_train_reshaped, y_train_encoded, X_val_reshaped, y_val_encoded, max_trials=200\n",
    ")\n",
    "\n",
    "# Convert best hyperparameters to config format\n",
    "best_config = {\n",
    "    'model_type': best_hps.get('model_type'),\n",
    "    'learning_rate': best_hps.get('learning_rate'),\n",
    "    'optimizer': best_hps.get('optimizer'),\n",
    "    'num_layers': best_hps.get('num_layers'),\n",
    "    'batch_size': 32  # Fixed during tuning\n",
    "}\n",
    "\n",
    "print(\"\\nBest configuration:\", best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef3b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the best model with more epochs for final evaluation\n",
    "print(\"=== RETRAINING BEST MODEL WITH MORE EPOCHS ===\")\n",
    "\n",
    "# Rebuild the best model with the found hyperparameters\n",
    "final_model = build_model(best_hps)\n",
    "\n",
    "# Train with more epochs and callbacks\n",
    "final_history = final_model.fit(\n",
    "    X_train_reshaped, y_train_encoded,\n",
    "    validation_data=(X_val_reshaped, y_val_encoded),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=12, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=6, factor=.5, min_lr=1e-7)\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d50730",
   "metadata": {},
   "source": [
    "## 8. Results and Analysis\n",
    "**Evaluation Metrics**:\n",
    "- Accuracy: Overall classification performance\n",
    "- Precision, Recall, F1-score: Per-class performance metrics\n",
    "- Confusion Matrix: Visual representation of classification patterns\n",
    "- Cross-validation: Robust performance estimation across different data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cffb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model on test set\n",
    "print(\"=== FINAL EVALUATION ON TEST SET (FOLD 10) ===\")\n",
    "\n",
    "test_loss, test_accuracy = final_model.evaluate(X_test_reshaped, y_test_encoded, verbose=0)\n",
    "y_pred = final_model.predict(X_test_reshaped)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Final Model Test Accuracy (Fold 10): %.4f\" % test_accuracy)\n",
    "print(\"Final Model Test Loss: %.4f\" % test_loss)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "class_report = classification_report(y_test_encoded, y_pred_classes,\n",
    "                          target_names=label_encoder.classes_, output_dict=True)\n",
    "print(classification_report(y_test_encoded, y_pred_classes,\n",
    "                          target_names=label_encoder.classes_))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test_encoded, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix - Final Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f3b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient 10-Fold Cross-Validation\n",
    "print(\"=== 10-FOLD CROSS-VALIDATION WITH BEST MODEL ===\")\n",
    "\n",
    "def run_efficient_cross_validation(all_features, all_labels, all_folds, best_config, n_folds=10):\n",
    "    \"\"\"Efficient cross-validation using precomputed features\"\"\"\n",
    "\n",
    "    fold_accuracies = []\n",
    "    label_encoder_cv = LabelEncoder()\n",
    "    labels_encoded = label_encoder_cv.fit_transform(all_labels)\n",
    "\n",
    "    for test_fold in range(1, n_folds + 1):\n",
    "        print(\"\\n--- Fold %d/%d ---\" % (test_fold, n_folds))\n",
    "\n",
    "        # Use current fold for testing, previous fold for validation\n",
    "        val_fold = test_fold - 1 if test_fold > 1 else n_folds\n",
    "        train_folds = [f for f in range(1, n_folds + 1) if f not in [test_fold, val_fold]]\n",
    "\n",
    "        # Get indices using precomputed features\n",
    "        train_mask = np.isin(all_folds, train_folds)\n",
    "        val_mask = all_folds == val_fold\n",
    "        test_mask = all_folds == test_fold\n",
    "\n",
    "        X_train_fold, y_train_fold = all_features[train_mask], labels_encoded[train_mask]\n",
    "        X_val_fold, y_val_fold = all_features[val_mask], labels_encoded[val_mask]\n",
    "        X_test_fold, y_test_fold = all_features[test_mask], labels_encoded[test_mask]\n",
    "\n",
    "        # Scale features per fold\n",
    "        scaler_fold = StandardScaler()\n",
    "        X_train_scaled_fold = scaler_fold.fit_transform(X_train_fold)\n",
    "        X_val_scaled_fold = scaler_fold.transform(X_val_fold)\n",
    "        X_test_scaled_fold = scaler_fold.transform(X_test_fold)\n",
    "\n",
    "        # Reshape for CNN/LSTM\n",
    "        X_train_reshaped_fold = X_train_scaled_fold.reshape(X_train_scaled_fold.shape[0], X_train_scaled_fold.shape[1], 1)\n",
    "        X_val_reshaped_fold = X_val_scaled_fold.reshape(X_val_scaled_fold.shape[0], X_val_scaled_fold.shape[1], 1)\n",
    "        X_test_reshaped_fold = X_test_scaled_fold.reshape(X_test_scaled_fold.shape[0], X_test_scaled_fold.shape[1], 1)\n",
    "\n",
    "        # Rebuild and train model for this fold\n",
    "        model_fold = build_model(best_hps)  # Use the same architecture but retrain\n",
    "\n",
    "        model_fold.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=best_config['learning_rate']),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        history_fold = model_fold.fit(\n",
    "            X_train_reshaped_fold, y_train_fold,\n",
    "            validation_data=(X_val_reshaped_fold, y_val_fold),\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            verbose=0,\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Evaluate on test fold\n",
    "        test_accuracy_fold = model_fold.evaluate(X_test_reshaped_fold, y_test_fold, verbose=0)[1]\n",
    "        fold_accuracies.append(test_accuracy_fold)\n",
    "        print(\"Fold %d Test Accuracy: %.4f\" % (test_fold, test_accuracy_fold))\n",
    "\n",
    "        # Clean up\n",
    "        del model_fold\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    return fold_accuracies\n",
    "\n",
    "# Run efficient cross-validation\n",
    "cv_accuracies = run_efficient_cross_validation(all_features, all_labels, all_folds, best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa35c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation Results Analysis\n",
    "print(\"\\n=== 10-FOLD CROSS-VALIDATION RESULTS ===\")\n",
    "print(\"Individual Fold Accuracies:\", ['%.4f' % acc for acc in cv_accuracies])\n",
    "print(\"Mean Accuracy: %.4f ± %.4f\" % (np.mean(cv_accuracies), np.std(cv_accuracies)))\n",
    "print(\"Min Accuracy: %.4f\" % np.min(cv_accuracies))\n",
    "print(\"Max Accuracy: %.4f\" % np.max(cv_accuracies))\n",
    "\n",
    "# Enhanced visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, 11), cv_accuracies, 'o-', linewidth=2, markersize=8, label='Fold Accuracy')\n",
    "plt.axhline(y=np.mean(cv_accuracies), color='r', linestyle='--', label='Mean: %.4f' % np.mean(cv_accuracies))\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('10-Fold Cross-Validation Results')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.boxplot(cv_accuracies)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Distribution Across Folds')\n",
    "plt.grid(True, alpha=.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "# Training history for the final model\n",
    "plt.plot(final_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(final_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Final Model Training History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c76b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Comparison with detailed analysis\n",
    "print(\"=== COMPREHENSIVE RESULTS ANALYSIS ===\")\n",
    "print(\"Model Selection Phase (Fold 10 Test): %.4f\" % test_accuracy)\n",
    "print(\"10-Fold Cross-Validation Mean: %.4f\" % np.mean(cv_accuracies))\n",
    "print(\"Performance Difference: %.4f\" % (test_accuracy - np.mean(cv_accuracies)))\n",
    "\n",
    "# Per-class performance analysis\n",
    "print(\"\\n=== PER-CLASS PERFORMANCE ANALYSIS ===\")\n",
    "class_performance = pd.DataFrame(class_report).transpose()\n",
    "print(class_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning Analysis\n",
    "print(\"Best Model Type:\", best_config['model_type'])\n",
    "print(\"Best Learning Rate:\", best_config['learning_rate'])\n",
    "print(\"Best Optimizer:\", best_config['optimizer'])\n",
    "print(\"Best Architecture Configuration:\", best_config)\n",
    "\n",
    "# Visualize hyperparameter search results\n",
    "tuner_results = tuner.oracle.get_best_trials(num_trials=10)\n",
    "tuner_data = []\n",
    "for trial in tuner_results:\n",
    "    tuner_data.append({**trial.hyperparameters.values, 'score': trial.score})\n",
    "\n",
    "tuner_df = pd.DataFrame(tuner_data)\n",
    "print(\"\\nTop 10 Hyperparameter Configurations:\")\n",
    "print(tuner_df.sort_values('score', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary Table\n",
    "pd.DataFrame({\n",
    "    'Evaluation Method': ['Single Test Fold (Fold 10)', '10-Fold Cross-Validation'],\n",
    "    'Accuracy': ['%.4f ± %.4f' % (test_accuracy, 0.), '%.4f ± %.4f' % (np.mean(cv_accuracies), np.std(cv_accuracies))],\n",
    "    'Best Model': [best_config['model_type'], best_config['model_type']],\n",
    "    'Learning Rate': [best_config['learning_rate'], best_config['learning_rate']]\n",
    "}).style.set_caption(\"Final performance summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ea47cb",
   "metadata": {},
   "source": [
    "## 9. Discussion and Conclusion\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "- Preprocessing failed on some of the much shorter clips. The issue was fixed by setting `mode='nearest'` to the two `librosa.feature.delta()` calls.\n",
    "- Feature extraction took too much time especially during the final training, where the designation of training, validation, and testing data was constantly changing. This was fixed by extracting features from each fold separately at the very beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42c497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
